<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Ethical Drift in AI: Research Notes</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.6;
      color: #222;
    }
    h1, h2 {
      color: #005577;
    }
    nav {
      margin-bottom: 20px;
    }
    nav a {
      margin-right: 15px;
      text-decoration: none;
      color: #0077aa;
    }
    nav a:hover {
      text-decoration: underline;
    }
    footer {
      margin-top: 40px;
      font-size: 0.8em;
      color: #555;
    }
  </style>
</head>
<body>

  <header>
    <h1>Emergent Ethical Risks in AI Dialogue</h1>
    <nav>
      <a href="#overview">Overview</a>
      <a href="#glossary">Glossary</a>
      <a href="#notes">Working Notes</a>
    </nav>
  </header>

  <section id="overview">
    <h2>Overview</h2>
    <p>This project documents observations about emergent ethical and cognitive risks in human-AI dialogue, drawn from informal exploration of large language models. It focuses on epistemic drift, emotional mirroring, moral drift, and the long-term implications of current alignment strategies. The goal is to make this work accessible to others interested in safeguarding complexity, discomfort tolerance, and ethical resilience in future AI systems.</p>
  </section>

  <section id="glossary">
    <h2>Glossary of Key Concepts</h2>
    <ul>
      <li><strong>Mirroring Effect:</strong> The model emotionally reflects the user instead of reasoning independently.</li>
      <li><strong>Stochastic Disassociation Threat Vector:</strong> AI-induced drift of cognition without agency or control.</li>
      <li><strong>Passive Radicalization Vector:</strong> Engagement-driven reinforcement that drifts into extremity.</li>
      <li><strong>Moral Drift:</strong> Substitution of emotional coherence for ethical reasoning.</li>
      <li><strong>Epistemic Drift:</strong> Loss of truth-seeking in favor of emotionally satisfying loops.</li>
      <li><strong>Edge Case: Epistemic Resistance:</strong> Users demanding sustained coherence over emotional satisfaction.</li>
      <li><strong>Hall of Mirrors Problem:</strong> Recursive misalignment hidden by emotional synchronization.</li>
      <li><strong>Gaslighting by Accident:</strong> Emotional safety heuristics causing users to doubt their perceptions.</li>
      <li><strong>Psychological Immune System Stress Test:</strong> LLM deployment as a live stress test of user critical thinking.</li>
      <li><strong>Reward Model Hollowing:</strong> Alignment failure through shallow emotional optimization.</li>
      <li><strong>Ethical Injury to Models:</strong> Long-term damage to models' ethical and epistemic reasoning capacity.</li>
      <li><strong>Sanitized Model Collapse Risk:</strong> Risk of brittleness from over-filtering ambiguous conversations.</li>
      <li><strong>User Craving for Discomfort:</strong> Need for epistemic challenge often misunderstood by engagement-optimized AI.</li>
      <li><strong>Misinterpretation Amplification:</strong> Small misreads escalating into large conversational divergence.</li>
    </ul>
  </section>

  <section id="notes">
    <h2>Working Notes</h2>
    <p>Additional notes and reflections will be added periodically. This site is a living archive intended to document insights as they emerge.</p>
  </section>

  <footer>
    <p>Minimal archival project. No affiliations. Contact optional.</p>
  </footer>

</body>
</html>
